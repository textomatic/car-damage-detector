{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Consider cropping the image in training to only include the bounding box\n",
    "# Seperate into classes and train the model on that\n",
    "# Then use a sliding window approach to detect predictions and bounding boxes\n",
    "# Then calculate the IoU/AP between the bounding boxes and the ground truth bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pickle\n",
    "import os\n",
    "import cv2\n",
    "import json\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from skimage.feature import hog\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "WINDOW_IMG_WIDTH = 96\n",
    "WINDOW_IMG_HEIGHT= 96\n",
    "CLASSES = ['Dent', 'Scratch', 'Crack', 'Glass shatter', 'Lamp broken', 'Tire flat']\n",
    "COLORS = {1: (255,0,0), 2: (0, 255, 0), 3: (0, 0, 255), 4: (255, 255, 0), 5: (255, 0, 255), 6: (0, 255, 255)}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############\n",
    "# FUNCTIONS #\n",
    "#############\n",
    "\n",
    "def load_train_data(image_dir, labels_dir):\n",
    "    # Generate the training and testing image paths\n",
    "    train_image_path = os.path.join(image_dir, 'train')\n",
    "    train_labels_path = os.path.join(labels_dir, 'train.json')\n",
    "    test_labels_path = os.path.join(labels_dir, 'test.json')\n",
    "    \n",
    "    # Process the labels from the JSON file\n",
    "    train_labels_dict = _extract_labels(train_labels_path)\n",
    "    test_labels_dict = _extract_labels(test_labels_path)\n",
    "    \n",
    "    # Get the training images and labels\n",
    "    train_images = []\n",
    "    train_labels = []\n",
    "    for image_file in os.listdir(train_image_path):\n",
    "        # Get the image ID\n",
    "        image_id = int(image_file.split('.')[0])\n",
    "        \n",
    "        # Load the image\n",
    "        image = cv2.imread(os.path.join(train_image_path, image_file))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # Ensure it is RGB\n",
    "        \n",
    "        # Crop the image at the bounding box for each label\n",
    "        for i, unique_label in enumerate(train_labels_dict['classification'][image_id]):\n",
    "            # Get the bounding box for the label\n",
    "            (x, y, w, h) = train_labels_dict['bbox'][image_id][i]\n",
    "\n",
    "            # Crop the image at the bounding box\n",
    "            cropped_image = image[int(np.floor(x)) : int(np.ceil(x+w)), int(np.floor(y)) : int(np.ceil(y+h))]\n",
    "            \n",
    "            # Ensure the width and height are big enough, otherwise it will cause problems\n",
    "            if(cropped_image.shape[0] <= 0 or cropped_image.shape[1] <= 0):\n",
    "                continue\n",
    "            # Resize image to a standard size\n",
    "            cropped_image = cv2.resize(cropped_image, (WINDOW_IMG_WIDTH, WINDOW_IMG_HEIGHT))\n",
    "            \n",
    "            # Extract the HOG features from the cropped image\n",
    "            img_features = hog(cropped_image, orientations=9, pixels_per_cell=(16,16), cells_per_block=(2,2), channel_axis=2)\n",
    "            \n",
    "            # Append the image and labels to the datasets\n",
    "            train_images.append(img_features)\n",
    "            train_labels.append(unique_label)  \n",
    "       \n",
    "    # Return the data \n",
    "    return np.array(train_images), np.array(train_labels), test_labels_dict\n",
    "\n",
    "def _extract_labels(labels_file):\n",
    "    with(open(labels_file)) as label_data:\n",
    "        annot_df = json.load(label_data)\n",
    "        \n",
    "    labels = {\n",
    "        \"classification\": {}, # Store image id and list of labels for that image\n",
    "        'bbox' : {},          # Store image id and list of bounding boxes for that image\n",
    "        'segementation' : {}  # Store image id and list of segementations for that image\n",
    "    }\n",
    "    \n",
    "    # Add labels to the dictionary\n",
    "    for annotations in annot_df['annotations']:\n",
    "        image_id = annotations['image_id']\n",
    "        \n",
    "        # Classification\n",
    "        if(image_id in labels['classification'].keys()):\n",
    "            labels['classification'][image_id].append(annotations['category_id'])\n",
    "        else:\n",
    "            labels['classification'][image_id] = [annotations['category_id']]\n",
    "            \n",
    "        # Bounding Box\n",
    "        if(image_id in labels['bbox'].keys()):\n",
    "            labels['bbox'][image_id].append(annotations['bbox'])\n",
    "        else:\n",
    "            labels['bbox'][image_id] = [annotations['bbox']]\n",
    "        \n",
    "        # Segmentation\n",
    "        if(image_id in labels['segementation'].keys()):\n",
    "            labels['segementation'][image_id].append(annotations['segmentation'])\n",
    "        else:\n",
    "            labels['segementation'][image_id] = [annotations['segmentation']]\n",
    "    \n",
    "    # Return the labels\n",
    "    return labels\n",
    "\n",
    "\n",
    "def train_model(images, labels, random_state=0):\n",
    "    # Instatite the model\n",
    "    model = SVC(probability=True, random_state=random_state)\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(images, labels)\n",
    "    \n",
    "    # Return the model\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, train_labels, test_labels = load_train_data(image_dir='../data/images', labels_dir='../data/images/annotations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6023, 900)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape\n",
    "#train_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_model(train_images, train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "import pickle\n",
    "model_file = '../models/SVM_MODEL.pkl'\n",
    "pickle.dump(model, open(model_file, 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the model for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############\n",
    "# FUNCTIONS #\n",
    "#############\n",
    "\n",
    "class Heatmap():\n",
    "    \n",
    "    def __init__(self,original_image):\n",
    "        \n",
    "        # Mask attribute is the heatmap initialized with zeros\n",
    "        self.mask = np.zeros(original_image.shape[:2])\n",
    "        self.labels = np.zeros(original_image.shape[:2])\n",
    "    \n",
    "    # Increase value of region function will add some heat to heatmap\n",
    "    def incValOfReg(self,coords, label):\n",
    "        w1,w2,h1,h2 = coords\n",
    "        self.mask[h1:h2,w1:w2] = self.mask[h1:h2,w1:w2] + 30\n",
    "        self.labels[h1:h2, w1:w2] = label\n",
    "    \n",
    "    # Decrease value of region function will remove some heat from heatmap\n",
    "    # We'll use this function if a region considered negative\n",
    "    def decValOfReg(self,coords):\n",
    "        w1,w2,h1,h2 = coords\n",
    "        self.mask[h1:h2,w1:w2] = self.mask[h1:h2,w1:w2] - 30\n",
    "    \n",
    "    def compileHeatmap(self):\n",
    "        \n",
    "        # As you know,pixel values must be between 0 and 255 (uint8)\n",
    "        # Now we'll scale our values between 0 and 255 and convert it to uint8\n",
    "        \n",
    "        # Scaling between 0 and 1 \n",
    "        scaler = MinMaxScaler()\n",
    "        \n",
    "        self.mask = scaler.fit_transform(self.mask)\n",
    "        \n",
    "        \n",
    "        # Scaling between 0 and 255\n",
    "        self.mask = np.asarray(self.mask * 255).astype(np.uint8)\n",
    "        \n",
    "        # Now we'll threshold our mask, if a value is higher than 170, it will be white else\n",
    "        # it will be black\n",
    "        self.mask = cv2.inRange(self.mask,170,255)\n",
    "        \n",
    "        return self.mask\n",
    "    \n",
    "# Adapted from https://www.kaggle.com/code/mehmetlaudatekman/support-vector-machine-object-detection\n",
    "def slideExtract(image, windowSize=(WINDOW_IMG_WIDTH, WINDOW_IMG_HEIGHT), step=12):\n",
    "    \n",
    "    # We'll store coords and features in these lists\n",
    "    coords = []\n",
    "    features = []\n",
    "    \n",
    "    hIm,wIm = image.shape[:2] \n",
    "\n",
    "    \n",
    "    # W1 will start from 0 to end of image - window size\n",
    "    # W2 will start from window size to end of image\n",
    "    # We'll use step (stride) like convolution kernels.\n",
    "    for w1,w2 in zip(range(0,wIm-windowSize[0],step),range(windowSize[0],wIm,step)):\n",
    "       \n",
    "        for h1,h2 in zip(range(0,hIm-windowSize[1],step),range(windowSize[1],hIm,step)):\n",
    "            window = image[h1:h2,w1:w2]\n",
    "            features_of_window = hog(window,orientations=9,pixels_per_cell=(16,16),\n",
    "                                     cells_per_block=(2,2), channel_axis=2\n",
    "                                    )\n",
    "            \n",
    "            coords.append((w1,w2,h1,h2))\n",
    "            features.append(features_of_window)\n",
    "    \n",
    "    return coords, np.asarray(features)\n",
    "\n",
    "def detection(model, val_images_dir, step=24, threshold=0.5):\n",
    "    pred_classes = {}\n",
    "    pred_bboxes = {}\n",
    "    \n",
    "    for count, file in enumerate(os.listdir(val_images_dir)):\n",
    "        # Open the image\n",
    "        image_id = int(file.split('.')[0])\n",
    "        image = np.asarray(Image.open(os.path.join(val_images_dir, file)))\n",
    "        \n",
    "        # Extracting features and initalizing heatmap\n",
    "        coords,features = slideExtract(image, step=step)\n",
    "        htmp = Heatmap(image)\n",
    "        \n",
    "        \n",
    "        for i in range(len(features)):\n",
    "            # If region is positive then add some heat\n",
    "            pred = model.predict_proba([features[i]])\n",
    "            if(max(pred[0]) > threshold):\n",
    "                htmp.incValOfReg(coords[i], np.argmax(pred[0]) + 1)\n",
    "            else:\n",
    "                htmp.decValOfReg(coords[i])\n",
    "        \n",
    "        # Compiling heatmap\n",
    "        mask = htmp.compileHeatmap()\n",
    "        \n",
    "        cont,_ = cv2.findContours(mask,1,2)[:2]\n",
    "        for c in cont:\n",
    "            # If a contour is small don't consider it\n",
    "            if cv2.contourArea(c) < 70*70:\n",
    "                continue\n",
    "            \n",
    "            (x,y,w,h) = cv2.boundingRect(c)\n",
    "            \n",
    "            if (htmp.labels[y,x] != 0):\n",
    "                if image_id in pred_bboxes.keys():\n",
    "                    pred_bboxes[image_id].append([x,y,w,h])\n",
    "                else:\n",
    "                    pred_bboxes[image_id] = [[x,y,w,h]]\n",
    "                    \n",
    "                if image_id in pred_classes.keys():\n",
    "                    pred_classes[image_id].append(htmp.labels[y,x])\n",
    "                else:\n",
    "                    pred_classes[image_id] = [htmp.labels[y,x]]\n",
    "        \n",
    "        if(count % 50 == 0):\n",
    "            print(f\"{count}/374 images processed\")\n",
    "                \n",
    "    return pred_classes, pred_bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model = pickle.load(open('../models/SVM_MODEL.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/374 images processed\n",
      "50/374 images processed\n",
      "100/374 images processed\n",
      "150/374 images processed\n",
      "200/374 images processed\n",
      "250/374 images processed\n",
      "300/374 images processed\n",
      "350/374 images processed\n"
     ]
    }
   ],
   "source": [
    "pred_classes, pred_bboxes = detection(model, val_images_dir='../data/images/test', step=36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1., 2., 4., 5., 6.]), array([717, 811,  32,   1,   4], dtype=int64))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = []\n",
    "for val in pred_classes.values():\n",
    "    x += val\n",
    "    \n",
    "np.unique(np.array(x), return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[78], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m val_labels \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([])\n\u001b[0;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m label \u001b[39min\u001b[39;00m test_labels[\u001b[39m'\u001b[39m\u001b[39mclassification\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mvalues():\n\u001b[1;32m----> 6\u001b[0m     val_labels \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mconcatenate(val_labels, np\u001b[39m.\u001b[39;49marray(label))\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "val_images_dir = \"../data/images/test\"\n",
    "val_images = {int(file.split('.')[0]):np.asarray(Image.open(os.path.join(val_images_dir, file))) for file in os.listdir(val_images_dir)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_labels = []\n",
    "for label in test_labels['classification'].values():\n",
    "    val_labels = val_labels + label\n",
    "\n",
    "val_labels = np.array(val_labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the AP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IoU(boxA, boxB):\n",
    "    # Resource: https://pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/\n",
    "    # determine the (x, y)-coordinates of the intersection rectangle\n",
    "\txA = max(boxA[0], boxB[0])\n",
    "\tyA = max(boxA[1], boxB[1])\n",
    "\txB = min(boxA[2], boxB[2])\n",
    "\tyB = min(boxA[3], boxB[3])\n",
    "\t# compute the area of intersection rectangle\n",
    "\tinterArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n",
    "\t# compute the area of both the prediction and ground-truth\n",
    "\t# rectangles\n",
    "\tboxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n",
    "\tboxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n",
    "\t# compute the intersection over union by taking the intersection\n",
    "\t# area and dividing it by the sum of prediction + ground-truth\n",
    "\t# areas - the interesection area\n",
    "\tiou = interArea / float(boxAArea + boxBArea - interArea)\n",
    "\t# return the intersection over union value\n",
    "\treturn iou\n",
    "\n",
    "def calculate_ap(true_classes, pred_classes, true_bboxes, pred_bboxes, threshold=0.5):\n",
    "    # Store the aps\n",
    "    class_aps = np.zeros(len(CLASSES))\n",
    "        \n",
    "        # Loop through the images\n",
    "          # Loop through each category in the image\n",
    "           # Loop through predited bounding boxes\n",
    "            # Loop through real bounding boxes\n",
    "                # If they match IoU and predition --> TP++\n",
    "                # Otherwise FP++\n",
    "           # Remaining are FN (need to multiply by 2) \n",
    "        # Calculate AP\n",
    "        \n",
    "        \n",
    "    # Track TP, FP, and FN for each class\n",
    "    class_TPS = np.zeros(len(CLASSES))\n",
    "    class_FPS = np.zeros(len(CLASSES))\n",
    "    class_FNS = np.zeros(len(CLASSES))\n",
    "        \n",
    "    # Loop through the images\n",
    "    for image_id in pred_classes.keys():\n",
    "        \n",
    "        # Get the true and predicted classes for this image\n",
    "        true_classes_image = true_classes[image_id]\n",
    "        pred_classes_image = pred_classes[image_id]\n",
    "        \n",
    "        # Get the true and predicted bboxes for this image\n",
    "        true_bboxes_image = true_bboxes[image_id]\n",
    "        pred_bboxes_image = pred_bboxes[image_id]\n",
    "    \n",
    "        # For each true category in the image\n",
    "        for cat in true_classes_image:\n",
    "            # If there are no predictions\n",
    "            if(len(pred_classes_image) == 0):\n",
    "                class_FNS[cat - 1] += len(true_classes_image)\n",
    "                continue\n",
    "            \n",
    "            # Calculate number of FNS\n",
    "            if(len(true_classes_image) > len(pred_classes_image)):\n",
    "                class_FNS[cat - 1] += len(true_classes_image) - len(pred_classes_image)\n",
    "            \n",
    "            # Calculate TPS and FPS\n",
    "            for j in range(len((true_bboxes_image))):\n",
    "                for k in range(j, len(pred_bboxes_image)):\n",
    "                    true_bbox = true_bboxes_image[j]\n",
    "                    pred_bbox = pred_bboxes_image[k]\n",
    "                    \n",
    "                    # Calculate IoU\n",
    "                    iou = IoU(true_bbox, pred_bbox)\n",
    "                    \n",
    "                    # If the IoU is greater than the threshold\n",
    "                    if(iou > threshold):\n",
    "                        # If the predicted class is the same as the true class\n",
    "                        if(int(pred_classes_image[k]) == cat):\n",
    "                            class_TPS[cat - 1] += 1\n",
    "                        else:\n",
    "                            class_FPS[cat - 1] += 1\n",
    "                    else:\n",
    "                        class_FPS[cat - 1] += 1\n",
    "        \n",
    "    for cat in range(len(CLASSES)):\n",
    "        # Calculate Precision and Recall \n",
    "        precision = np.nan_to_num(class_TPS[cat] / (class_TPS[cat] + class_FPS[cat]), nan=0.1, posinf=1, neginf=0)\n",
    "        recall = np.nan_to_num(class_TPS[cat] / (class_TPS[cat] + class_FNS[cat]), nan=0.1, posinf=1, neginf=0)\n",
    "        class_aps[cat] += np.nan_to_num(precision*recall, nan=0.1, posinf=1, neginf=0)\n",
    "    \n",
    "    # Return the mean AP for each class\n",
    "    return class_aps\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Average Precision (AP) for each class\n",
    "class_aps = calculate_ap(true_classes=test_labels['classification'], pred_classes=pred_classes, true_bboxes=test_labels['bbox'], pred_bboxes=pred_bboxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold: 0.1\n",
      "0/374 images processed\n",
      "50/374 images processed\n",
      "100/374 images processed\n",
      "150/374 images processed\n",
      "200/374 images processed\n",
      "250/374 images processed\n",
      "300/374 images processed\n",
      "350/374 images processed\n",
      "Threshold: 0.30000000000000004\n",
      "0/374 images processed\n",
      "50/374 images processed\n",
      "100/374 images processed\n",
      "150/374 images processed\n",
      "200/374 images processed\n",
      "250/374 images processed\n",
      "300/374 images processed\n",
      "350/374 images processed\n",
      "Threshold: 0.5000000000000001\n",
      "0/374 images processed\n",
      "50/374 images processed\n",
      "100/374 images processed\n",
      "150/374 images processed\n",
      "200/374 images processed\n",
      "250/374 images processed\n",
      "300/374 images processed\n",
      "350/374 images processed\n",
      "Threshold: 0.7000000000000001\n",
      "0/374 images processed\n",
      "50/374 images processed\n",
      "100/374 images processed\n",
      "150/374 images processed\n",
      "200/374 images processed\n",
      "250/374 images processed\n",
      "300/374 images processed\n",
      "350/374 images processed\n",
      "Threshold: 0.9000000000000001\n",
      "0/374 images processed\n",
      "50/374 images processed\n",
      "100/374 images processed\n",
      "150/374 images processed\n",
      "200/374 images processed\n",
      "250/374 images processed\n",
      "300/374 images processed\n",
      "350/374 images processed\n"
     ]
    }
   ],
   "source": [
    "class_aps = np.zeros(len(CLASSES))\n",
    "\n",
    "for threshold in np.arange(0.1, 1, 0.2):\n",
    "    print(f\"Threshold: {threshold}\")\n",
    "    pred_classes, pred_bboxes = detection(model, val_images_dir='../data/images/test', step=36, threshold=threshold)\n",
    "    class_aps += (calculate_ap(true_classes=test_labels['classification'], pred_classes=pred_classes, true_bboxes=test_labels['bbox'], pred_bboxes=pred_bboxes, threshold=threshold)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([23.0516841 , 26.37819468, 20.        , 20.45913682, 20.        ,\n",
       "       20.        ])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_aps * 100 / 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 6]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels['classification'][12]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIPI540",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "364c5602cd615be67ab979cb13b1a8b8123ba748870cd148a993275c319eca87"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
